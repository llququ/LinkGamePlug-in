{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import image\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "\n",
    "data_shape = 448\n",
    "batch_size = 4\n",
    "rgb_mean = nd.array([123, 117, 104])\n",
    "rgb_std = nd.array([58.395, 57.12, 57.375])\n",
    "\n",
    "def get_iterators(data_shape, batch_size):\n",
    "    class_names = ['banana','blueberry','grape','peach','pineapple','strawberry']\n",
    "    num_class = len(class_names)\n",
    "    train_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        #path_imgrec=data_dir+'train.rec',\n",
    "        #path_imgidx=data_dir+'train.idx',\n",
    "        path_imglist='D:/1/test.lst',\n",
    "        path_root='D:/1/data/',\n",
    "        shuffle=True,\n",
    "        mean=True,\n",
    "        std=True,\n",
    "        #rand_crop=1,\n",
    "        min_object_covered=0.95,\n",
    "        max_attempts=200\n",
    "    )\n",
    "    '''\n",
    "    val_iter = image.ImageDetIter(\n",
    "        batch_size=batch_size,\n",
    "        data_shape=(3, data_shape, data_shape),\n",
    "        path_imgrec=data_dir+'val.rec',\n",
    "        shuffle=False,\n",
    "        mean=True,\n",
    "        std=True)\n",
    "        '''\n",
    "    #return train_iter, val_iter, class_names, num_class\n",
    "    return train_iter, class_names, num_class\n",
    "#train_data, test_data, class_names, num_class = get_iterators(\n",
    "#    data_shape, batch_size)\n",
    "train_data, class_names, num_class = get_iterators(data_shape, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "data_iter = mx.image.ImageDetIter(batch_size=2, data_shape=(3,1024, 1024),path_imglist='D:/1/test.lst',path_root='D:/1/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 3L, 1024L, 1024L)\n",
      "\n",
      "[[ 1.          0.17152777  0.33828124  0.31874999  0.42226562]\n",
      " [ 1.          0.33958334  0.33867186  0.48819444  0.42265624]\n",
      " [ 1.          0.17152777  0.61914062  0.31874999  0.70351565]\n",
      " [ 1.          0.3361111   0.61874998  0.4861111   0.70351565]\n",
      " [ 0.          0.50416666  0.61874998  0.65555555  0.70156252]\n",
      " [ 0.          0.1701389   0.5277344   0.32361111  0.60976565]\n",
      " [ 0.          0.50277776  0.52656251  0.65416664  0.60859376]\n",
      " [ 0.          0.50208336  0.33828124  0.65416664  0.42109376]\n",
      " [ 5.          0.6722222   0.33945313  0.81944442  0.42109376]\n",
      " [ 5.          0.34097221  0.43320313  0.48402777  0.51601565]\n",
      " [ 5.          0.67083335  0.52539062  0.81805557  0.61171877]\n",
      " [ 5.          0.6701389   0.62031251  0.81805557  0.70429689]\n",
      " [ 3.          0.17222223  0.43125001  0.31874999  0.51601565]\n",
      " [ 3.          0.3361111   0.52460939  0.48333332  0.61093748]\n",
      " [ 3.          0.50486112  0.43203124  0.65208334  0.51640624]\n",
      " [ 3.          0.67291665  0.43203124  0.82013887  0.51757812]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]\n",
      " [-1.         -1.         -1.         -1.         -1.        ]]\n",
      "<NDArray 24x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "data_iter.reset()\n",
    "for data in data_iter:\n",
    "    d = data.data[0]\n",
    "    l = data.label[0]\n",
    "    print(d.shape)\n",
    "    print(l[0])\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.17152777  0.33828124  0.31874999  0.42226562]\n",
      "[ 1.          0.33958334  0.33867186  0.48819444  0.42265624]\n",
      "[ 1.          0.17152777  0.61914062  0.31874999  0.70351565]\n",
      "[ 1.          0.3361111   0.61874998  0.4861111   0.70351565]\n",
      "[ 0.          0.50416666  0.61874998  0.65555555  0.70156252]\n",
      "[ 0.          0.1701389   0.5277344   0.32361111  0.60976565]\n",
      "[ 0.          0.50277776  0.52656251  0.65416664  0.60859376]\n",
      "[ 0.          0.50208336  0.33828124  0.65416664  0.42109376]\n",
      "[ 5.          0.6722222   0.33945313  0.81944442  0.42109376]\n",
      "[ 5.          0.34097221  0.43320313  0.48402777  0.51601565]\n",
      "[ 5.          0.67083335  0.52539062  0.81805557  0.61171877]\n",
      "[ 5.          0.6701389   0.62031251  0.81805557  0.70429689]\n",
      "[ 3.          0.17222223  0.43125001  0.31874999  0.51601565]\n",
      "[ 3.          0.3361111   0.52460939  0.48333332  0.61093748]\n",
      "[ 3.          0.50486112  0.43203124  0.65208334  0.51640624]\n",
      "[ 3.          0.67291665  0.43203124  0.82013887  0.51757812]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "idx = 0\n",
    "img = d[0].asnumpy()  # grab the first image, convert to numpy array\n",
    "img = img.transpose((1, 2, 0))  # we want channel to be the last dimension #img += np.array([123, 117, 104])\n",
    "img = img.astype(np.uint8)  # use uint8 (0-255)\n",
    "for ll in l[0].asnumpy():\n",
    "    if ll[0]<0: #-1是补充的，不是label\n",
    "        break\n",
    "    else:\n",
    "        print(ll)\n",
    "        xmin= int(ll[1]*1024) #因为我的x的shape是734\n",
    "        ymin= int(ll[2]*1024) #因为我的y的shape是804\n",
    "        xmax= int(ll[3]*1024) \n",
    "        ymax = int(ll[4]*1024) \n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, edgecolor=(1, 0, 0), linewidth=1)\n",
    "        plt.gca().add_patch(rect)\n",
    "        #break\n",
    " \n",
    "plt.imshow(img,'gray')# 我的是灰度图，如果你们是彩色图也可以做别的操作\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "import numpy as np\n",
    "class YOLO2Output(nn.HybridBlock):\n",
    "    def __init__(self, num_class, anchor_scales, **kwargs):\n",
    "        super(YOLO2Output, self).__init__(**kwargs)\n",
    "        assert num_class > 0, \"number of classes should > 0, given {}\".format(num_class)\n",
    "        self._num_class = num_class\n",
    "        assert isinstance(anchor_scales, (list, tuple)), \"list or tuple of anchor scales required\"\n",
    "        assert len(anchor_scales) > 0, \"at least one anchor scale required\"\n",
    "        for anchor in anchor_scales:\n",
    "            assert len(anchor) == 2, \"expected each anchor scale to be (width, height), provided {}\".format(anchor)\n",
    "        self._anchor_scales = anchor_scales\n",
    "        out_channels = len(anchor_scales) * (num_class + 1 + 4)\n",
    "        with self.name_scope():\n",
    "            self.output = nn.Conv2D(out_channels, 1, 1)\n",
    "\n",
    "    def hybrid_forward(self, F, x, *args):\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.model_zoo import vision\n",
    "pretrained = vision.get_model('resnet18_v1', pretrained=True).features\n",
    "net = nn.HybridSequential()\n",
    "for i in range(len(pretrained) - 2):\n",
    "    net.add(pretrained[i])\n",
    "\n",
    "# anchor scales, try adjust it yourself\n",
    "scales = [[3.3004, 3.59034],\n",
    "          [9.84923, 8.23783]]\n",
    "\n",
    "# use 2 classes, 1 as dummy class, otherwise softmax won't work\n",
    "predictor = YOLO2Output(num_class, scales)\n",
    "predictor.initialize()\n",
    "net.add(predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "  (2): Activation(relu)\n",
      "  (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "  (4): HybridSequential(\n",
      "    (0): BasicBlockV1(\n",
      "      (body): HybridSequential(\n",
      "        (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        (2): Activation(relu)\n",
      "        (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlockV1(\n",
      "      (body): HybridSequential(\n",
      "        (0): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "        (2): Activation(relu)\n",
      "        (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=64)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): HybridSequential(\n",
      "    (0): BasicBlockV1(\n",
      "      (body): HybridSequential(\n",
      "        (0): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        (2): Activation(relu)\n",
      "        (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "      )\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlockV1(\n",
      "      (body): HybridSequential(\n",
      "        (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "        (2): Activation(relu)\n",
      "        (3): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=128)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): HybridSequential(\n",
      "    (0): BasicBlockV1(\n",
      "      (body): HybridSequential(\n",
      "        (0): Conv2D(128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        (2): Activation(relu)\n",
      "        (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "      )\n",
      "      (downsample): HybridSequential(\n",
      "        (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlockV1(\n",
      "      (body): HybridSequential(\n",
      "        (0): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "        (2): Activation(relu)\n",
      "        (3): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm(fix_gamma=False, use_global_stats=False, eps=1e-05, momentum=0.9, axis=1, in_channels=256)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): YOLO2Output(\n",
      "    (output): Conv2D(None -> 22, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def yolo2_forward(x, num_class, anchor_scales):\n",
    "    \"\"\"Transpose/reshape/organize convolution outputs.\"\"\"\n",
    "    stride = num_class + 5\n",
    "    # transpose and reshape, 4th dim is the number of anchors\n",
    "    x = x.transpose((0, 2, 3, 1))\n",
    "    x = x.reshape((0, 0, 0, -1, stride))\n",
    "    # now x is (batch, m, n, stride), stride = num_class + 1(object score) + 4(coordinates)\n",
    "    # class probs\n",
    "    cls_pred = x.slice_axis(begin=0, end=num_class, axis=-1)\n",
    "    # object score\n",
    "    score_pred = x.slice_axis(begin=num_class, end=num_class + 1, axis=-1)\n",
    "    score = nd.sigmoid(score_pred)\n",
    "    # center prediction, in range(0, 1) for each grid\n",
    "    xy_pred = x.slice_axis(begin=num_class + 1, end=num_class + 3, axis=-1)\n",
    "    xy = nd.sigmoid(xy_pred)\n",
    "    # width/height prediction\n",
    "    wh = x.slice_axis(begin=num_class + 3, end=num_class + 5, axis=-1)\n",
    "    # convert x, y to positions relative to image\n",
    "    x, y = transform_center(xy)\n",
    "    # convert w, h to width/height relative to image\n",
    "    w, h = transform_size(wh, anchor_scales)\n",
    "    # cid is the argmax channel\n",
    "    cid = nd.argmax(cls_pred, axis=-1, keepdims=True)\n",
    "    # convert to corner format boxes\n",
    "    half_w = w / 2\n",
    "    half_h = h / 2\n",
    "    left = nd.clip(x - half_w, 0, 1)\n",
    "    top = nd.clip(y - half_h, 0, 1)\n",
    "    right = nd.clip(x + half_w, 0, 1)\n",
    "    bottom = nd.clip(y + half_h, 0, 1)\n",
    "    output = nd.concat(*[cid, score, left, top, right, bottom], dim=4)\n",
    "    return output, cls_pred, score, nd.concat(*[xy, wh], dim=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_center(xy):\n",
    "    \"\"\"Given x, y prediction after sigmoid(), convert to relative coordinates (0, 1) on image.\"\"\"\n",
    "    b, h, w, n, s = xy.shape\n",
    "    offset_y = nd.tile(nd.arange(0, h, repeat=(w * n * 1), ctx=xy.context).reshape((1, h, w, n, 1)), (b, 1, 1, 1, 1))\n",
    "    # print(offset_y[0].asnumpy()[:, :, 0, 0])\n",
    "    offset_x = nd.tile(nd.arange(0, w, repeat=(n * 1), ctx=xy.context).reshape((1, 1, w, n, 1)), (b, h, 1, 1, 1))\n",
    "    # print(offset_x[0].asnumpy()[:, :, 0, 0])\n",
    "    x, y = xy.split(num_outputs=2, axis=-1)\n",
    "    x = (x + offset_x) / w\n",
    "    y = (y + offset_y) / h\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_size(wh, anchors):\n",
    "    \"\"\"Given w, h prediction after exp() and anchor sizes, convert to relative width/height (0, 1) on image\"\"\"\n",
    "    b, h, w, n, s = wh.shape\n",
    "    aw, ah = nd.tile(nd.array(anchors, ctx=wh.context).reshape((1, 1, 1, -1, 2)), (b, h, w, 1, 1)).split(num_outputs=2, axis=-1)\n",
    "    w_pred, h_pred = nd.exp(wh).split(num_outputs=2, axis=-1)\n",
    "    w_out = w_pred * aw / w\n",
    "    h_out = h_pred * ah / h\n",
    "    return w_out, h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yolo2_target(scores, boxes, labels, anchors, ignore_label=-1, thresh=0.5):\n",
    "    \"\"\"Generate training targets given predictions and labels.\"\"\"\n",
    "    b, h, w, n, _ = scores.shape\n",
    "    anchors = np.reshape(np.array(anchors), (-1, 2))\n",
    "    #scores = nd.slice_axis(outputs, begin=1, end=2, axis=-1)\n",
    "    #boxes = nd.slice_axis(outputs, begin=2, end=6, axis=-1)\n",
    "    gt_boxes = nd.slice_axis(labels, begin=1, end=5, axis=-1)\n",
    "    target_score = nd.zeros((b, h, w, n, 1), ctx=scores.context)\n",
    "    target_id = nd.ones_like(target_score, ctx=scores.context) * ignore_label\n",
    "    target_box = nd.zeros((b, h, w, n, 4), ctx=scores.context)\n",
    "    sample_weight = nd.zeros((b, h, w, n, 1), ctx=scores.context)\n",
    "    for b in range(output.shape[0]):\n",
    "        # find the best match for each ground-truth\n",
    "        label = labels[b].asnumpy()\n",
    "        valid_label = label[np.where(label[:, 0] > -0.5)[0], :]\n",
    "        # shuffle because multi gt could possibly match to one anchor, we keep the last match randomly\n",
    "        np.random.shuffle(valid_label)\n",
    "        for l in valid_label:\n",
    "            gx, gy, gw, gh = (l[1] + l[3]) / 2, (l[2] + l[4]) / 2, l[3] - l[1], l[4] - l[2]\n",
    "            ind_x = int(gx * w)\n",
    "            ind_y = int(gy * h)\n",
    "            tx = gx * w - ind_x\n",
    "            ty = gy * h - ind_y\n",
    "            gw = gw * w\n",
    "            gh = gh * h\n",
    "            # find the best match using width and height only, assuming centers are identical\n",
    "            intersect = np.minimum(anchors[:, 0], gw) * np.minimum(anchors[:, 1], gh)\n",
    "            ovps = intersect / (gw * gh + anchors[:, 0] * anchors[:, 1] - intersect)\n",
    "            best_match = int(np.argmax(ovps))\n",
    "            target_id[b, ind_y, ind_x, best_match, :] = l[0]\n",
    "            target_score[b, ind_y, ind_x, best_match, :] = 1.0\n",
    "            tw = np.log(gw / anchors[best_match, 0])\n",
    "            th = np.log(gh / anchors[best_match, 1])\n",
    "            target_box[b, ind_y, ind_x, best_match, :] = mx.nd.array([tx, ty, tw, th])\n",
    "            sample_weight[b, ind_y, ind_x, best_match, :] = 1.0\n",
    "            # print('ind_y', ind_y, 'ind_x', ind_x, 'best_match', best_match, 't', tx, ty, tw, th, 'ovp', ovps[best_match], 'gt', gx, gy, gw/w, gh/h, 'anchor', anchors[best_match, 0], anchors[best_match, 1])\n",
    "    return target_id, target_score, target_box, sample_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sce_loss = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=False)\n",
    "l1_loss = gluon.loss.L1Loss()\n",
    "\n",
    "from mxnet import metric\n",
    "\n",
    "class LossRecorder(mx.metric.EvalMetric):\n",
    "    \"\"\"LossRecorder is used to record raw loss so we can observe loss directly\n",
    "    \"\"\"\n",
    "    def __init__(self, name):\n",
    "        super(LossRecorder, self).__init__(name)\n",
    "\n",
    "    def update(self, labels, preds=0):\n",
    "        \"\"\"Update metric with pure loss\n",
    "        \"\"\"\n",
    "        for loss in labels:\n",
    "            if isinstance(loss, mx.nd.NDArray):\n",
    "                loss = loss.asnumpy()\n",
    "            self.sum_metric += loss.sum()\n",
    "            self.num_inst += 1\n",
    "\n",
    "obj_loss = LossRecorder('objectness_loss')\n",
    "cls_loss = LossRecorder('classification_loss')\n",
    "box_loss = LossRecorder('box_refine_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0, train  0.01416,  0.06110,  0.00557 time 84.3 sec\n",
      "Epoch  1, train  0.01410,  0.06088,  0.00542 time 70.5 sec\n",
      "Epoch  2, train  0.01407,  0.06070,  0.00531 time 73.4 sec\n",
      "Epoch  3, train  0.01404,  0.06052,  0.00521 time 74.3 sec\n",
      "Epoch  4, train  0.01396,  0.06030,  0.00511 time 70.1 sec\n",
      "Epoch  5, train  0.01403,  0.06019,  0.00506 time 69.4 sec\n",
      "Epoch  6, train  0.01393,  0.05996,  0.00497 time 68.3 sec\n",
      "Epoch  7, train  0.01394,  0.05980,  0.00492 time 68.1 sec\n",
      "Epoch  8, train  0.01388,  0.05960,  0.00485 time 68.8 sec\n",
      "Epoch  9, train  0.01389,  0.05945,  0.00482 time 67.3 sec\n",
      "Epoch 10, train  0.01381,  0.05923,  0.00476 time 65.2 sec\n",
      "Epoch 11, train  0.01376,  0.05903,  0.00471 time 65.0 sec\n",
      "Epoch 12, train  0.01374,  0.05885,  0.00469 time 65.0 sec\n",
      "Epoch 13, train  0.01375,  0.05871,  0.00467 time 65.0 sec\n",
      "Epoch 14, train  0.01372,  0.05851,  0.00463 time 65.3 sec\n",
      "Epoch 15, train  0.01365,  0.05831,  0.00460 time 65.1 sec\n",
      "Epoch 16, train  0.01363,  0.05814,  0.00458 time 64.9 sec\n",
      "Epoch 17, train  0.01360,  0.05796,  0.00455 time 65.1 sec\n",
      "Epoch 18, train  0.01359,  0.05779,  0.00454 time 65.7 sec\n",
      "Epoch 19, train  0.01353,  0.05759,  0.00451 time 65.3 sec\n",
      "Epoch 20, train  0.01355,  0.05744,  0.00450 time 65.4 sec\n",
      "Epoch 21, train  0.01349,  0.05724,  0.00447 time 65.4 sec\n",
      "Epoch 22, train  0.01344,  0.05704,  0.00445 time 65.5 sec\n",
      "Epoch 23, train  0.01344,  0.05689,  0.00444 time 65.4 sec\n",
      "Epoch 24, train  0.01341,  0.05671,  0.00442 time 65.1 sec\n",
      "Epoch 25, train  0.01333,  0.05649,  0.00439 time 65.1 sec\n",
      "Epoch 26, train  0.01333,  0.05633,  0.00437 time 65.2 sec\n",
      "Epoch 27, train  0.01325,  0.05611,  0.00434 time 64.9 sec\n",
      "Epoch 28, train  0.01325,  0.05596,  0.00433 time 65.1 sec\n",
      "Epoch 29, train  0.01325,  0.05580,  0.00433 time 65.0 sec\n",
      "Epoch 30, train  0.01319,  0.05560,  0.00430 time 65.1 sec\n",
      "Epoch 31, train  0.01313,  0.05540,  0.00428 time 65.3 sec\n",
      "Epoch 32, train  0.01314,  0.05526,  0.00427 time 65.2 sec\n",
      "Epoch 33, train  0.01312,  0.05507,  0.00426 time 65.3 sec\n",
      "Epoch 34, train  0.01307,  0.05487,  0.00424 time 65.1 sec\n",
      "Epoch 35, train  0.01306,  0.05470,  0.00423 time 65.3 sec\n",
      "Epoch 36, train  0.01302,  0.05452,  0.00422 time 65.5 sec\n",
      "Epoch 37, train  0.01299,  0.05435,  0.00420 time 65.2 sec\n",
      "Epoch 38, train  0.01291,  0.05413,  0.00418 time 65.1 sec\n",
      "Epoch 39, train  0.01295,  0.05400,  0.00419 time 65.0 sec\n",
      "Epoch 40, train  0.01286,  0.05378,  0.00416 time 65.0 sec\n",
      "Epoch 41, train  0.01284,  0.05360,  0.00415 time 65.1 sec\n",
      "Epoch 42, train  0.01284,  0.05346,  0.00414 time 65.2 sec\n",
      "Epoch 43, train  0.01282,  0.05327,  0.00413 time 65.3 sec\n",
      "Epoch 44, train  0.01272,  0.05302,  0.00410 time 65.3 sec\n",
      "Epoch 45, train  0.01268,  0.05286,  0.00409 time 65.1 sec\n",
      "Epoch 46, train  0.01265,  0.05267,  0.00408 time 65.2 sec\n",
      "Epoch 47, train  0.01262,  0.05249,  0.00407 time 65.2 sec\n",
      "Epoch 48, train  0.01260,  0.05233,  0.00406 time 65.2 sec\n",
      "Epoch 49, train  0.01253,  0.05212,  0.00404 time 65.2 sec\n",
      "Epoch 50, train  0.01250,  0.05195,  0.00404 time 65.2 sec\n",
      "Epoch 51, train  0.01250,  0.05177,  0.00404 time 65.1 sec\n",
      "Epoch 52, train  0.01245,  0.05157,  0.00402 time 65.1 sec\n",
      "Epoch 53, train  0.01243,  0.05142,  0.00402 time 65.0 sec\n",
      "Epoch 54, train  0.01235,  0.05122,  0.00400 time 65.1 sec\n",
      "Epoch 55, train  0.01238,  0.05107,  0.00401 time 65.1 sec\n",
      "Epoch 56, train  0.01235,  0.05089,  0.00400 time 65.1 sec\n",
      "Epoch 57, train  0.01229,  0.05069,  0.00398 time 65.3 sec\n",
      "Epoch 58, train  0.01224,  0.05053,  0.00397 time 65.3 sec\n",
      "Epoch 59, train  0.01225,  0.05036,  0.00398 time 65.0 sec\n",
      "Epoch 60, train  0.01220,  0.05016,  0.00397 time 65.2 sec\n",
      "Epoch 61, train  0.01214,  0.04997,  0.00395 time 65.2 sec\n",
      "Epoch 62, train  0.01211,  0.04980,  0.00394 time 65.3 sec\n",
      "Epoch 63, train  0.01206,  0.04960,  0.00393 time 65.3 sec\n",
      "Epoch 64, train  0.01203,  0.04943,  0.00393 time 65.0 sec\n",
      "Epoch 65, train  0.01203,  0.04928,  0.00393 time 65.0 sec\n",
      "Epoch 66, train  0.01200,  0.04909,  0.00392 time 65.2 sec\n",
      "Epoch 67, train  0.01196,  0.04892,  0.00392 time 65.1 sec\n",
      "Epoch 68, train  0.01190,  0.04872,  0.00390 time 65.1 sec\n",
      "Epoch 69, train  0.01186,  0.04853,  0.00390 time 65.2 sec\n",
      "Epoch 70, train  0.01187,  0.04839,  0.00391 time 65.3 sec\n",
      "Epoch 71, train  0.01180,  0.04819,  0.00389 time 65.2 sec\n",
      "Epoch 72, train  0.01181,  0.04807,  0.00390 time 65.2 sec\n",
      "Epoch 73, train  0.01175,  0.04785,  0.00388 time 65.2 sec\n",
      "Epoch 74, train  0.01170,  0.04769,  0.00387 time 65.2 sec\n",
      "Epoch 75, train  0.01170,  0.04752,  0.00388 time 65.1 sec\n",
      "Epoch 76, train  0.01165,  0.04734,  0.00387 time 65.1 sec\n",
      "Epoch 77, train  0.01159,  0.04716,  0.00386 time 64.9 sec\n",
      "Epoch 78, train  0.01154,  0.04697,  0.00384 time 65.1 sec\n",
      "Epoch 79, train  0.01155,  0.04685,  0.00385 time 65.0 sec\n",
      "Epoch 80, train  0.01151,  0.04665,  0.00384 time 65.1 sec\n",
      "Epoch 81, train  0.01150,  0.04651,  0.00385 time 65.2 sec\n",
      "Epoch 82, train  0.01144,  0.04631,  0.00383 time 65.1 sec\n",
      "Epoch 83, train  0.01144,  0.04618,  0.00384 time 65.2 sec\n",
      "Epoch 84, train  0.01137,  0.04600,  0.00382 time 65.2 sec\n",
      "Epoch 85, train  0.01132,  0.04577,  0.00381 time 65.1 sec\n",
      "Epoch 86, train  0.01128,  0.04561,  0.00381 time 65.1 sec\n",
      "Epoch 87, train  0.01129,  0.04548,  0.00382 time 67.5 sec\n",
      "Epoch 88, train  0.01122,  0.04530,  0.00380 time 67.9 sec\n",
      "Epoch 89, train  0.01122,  0.04514,  0.00381 time 67.5 sec\n",
      "Epoch 90, train  0.01116,  0.04496,  0.00379 time 67.5 sec\n",
      "Epoch 91, train  0.01110,  0.04481,  0.00378 time 67.3 sec\n",
      "Epoch 92, train  0.01112,  0.04467,  0.00379 time 66.8 sec\n",
      "Epoch 93, train  0.01109,  0.04451,  0.00379 time 64.9 sec\n",
      "Epoch 94, train  0.01105,  0.04433,  0.00379 time 64.9 sec\n",
      "Epoch 95, train  0.01104,  0.04419,  0.00378 time 65.1 sec\n",
      "Epoch 96, train  0.01099,  0.04401,  0.00378 time 65.0 sec\n",
      "Epoch 97, train  0.01091,  0.04380,  0.00376 time 65.1 sec\n",
      "Epoch 98, train  0.01092,  0.04369,  0.00377 time 65.2 sec\n",
      "Epoch 99, train  0.01085,  0.04349,  0.00375 time 65.1 sec\n",
      "Epoch 100, train  0.01082,  0.04335,  0.00375 time 65.0 sec\n",
      "Epoch 101, train  0.01078,  0.04320,  0.00374 time 65.2 sec\n",
      "Epoch 102, train  0.01073,  0.04302,  0.00373 time 65.3 sec\n",
      "Epoch 103, train  0.01072,  0.04287,  0.00374 time 65.2 sec\n",
      "Epoch 104, train  0.01068,  0.04272,  0.00373 time 65.0 sec\n",
      "Epoch 105, train  0.01066,  0.04257,  0.00373 time 65.2 sec\n",
      "Epoch 106, train  0.01059,  0.04241,  0.00372 time 65.3 sec\n",
      "Epoch 107, train  0.01059,  0.04223,  0.00372 time 65.0 sec\n",
      "Epoch 108, train  0.01055,  0.04209,  0.00371 time 65.0 sec\n",
      "Epoch 109, train  0.01052,  0.04194,  0.00371 time 65.1 sec\n",
      "Epoch 110, train  0.01049,  0.04181,  0.00371 time 65.1 sec\n",
      "Epoch 111, train  0.01044,  0.04163,  0.00370 time 65.2 sec\n",
      "Epoch 112, train  0.01040,  0.04150,  0.00370 time 65.2 sec\n",
      "Epoch 113, train  0.01039,  0.04132,  0.00370 time 65.2 sec\n",
      "Epoch 114, train  0.01037,  0.04119,  0.00369 time 65.1 sec\n",
      "Epoch 115, train  0.01028,  0.04101,  0.00367 time 65.1 sec\n",
      "Epoch 116, train  0.01030,  0.04092,  0.00369 time 65.1 sec\n",
      "Epoch 117, train  0.01028,  0.04079,  0.00369 time 65.2 sec\n",
      "Epoch 118, train  0.01020,  0.04060,  0.00367 time 65.1 sec\n",
      "Epoch 119, train  0.01015,  0.04042,  0.00366 time 64.9 sec\n",
      "Epoch 120, train  0.01012,  0.04031,  0.00366 time 65.1 sec\n",
      "Epoch 121, train  0.01011,  0.04015,  0.00366 time 65.2 sec\n",
      "Epoch 122, train  0.01010,  0.04002,  0.00367 time 65.1 sec\n",
      "Epoch 123, train  0.01004,  0.03984,  0.00365 time 65.3 sec\n",
      "Epoch 124, train  0.01002,  0.03975,  0.00366 time 65.3 sec\n",
      "Epoch 125, train  0.00999,  0.03958,  0.00365 time 65.3 sec\n",
      "Epoch 126, train  0.00996,  0.03946,  0.00365 time 65.4 sec\n",
      "Epoch 127, train  0.00993,  0.03932,  0.00365 time 65.2 sec\n",
      "Epoch 128, train  0.00987,  0.03915,  0.00363 time 65.0 sec\n",
      "Epoch 129, train  0.00983,  0.03900,  0.00363 time 64.9 sec\n",
      "Epoch 130, train  0.00981,  0.03886,  0.00362 time 64.9 sec\n",
      "Epoch 131, train  0.00977,  0.03871,  0.00362 time 65.1 sec\n",
      "Epoch 132, train  0.00971,  0.03858,  0.00361 time 65.1 sec\n",
      "Epoch 133, train  0.00966,  0.03842,  0.00360 time 65.2 sec\n",
      "Epoch 134, train  0.00964,  0.03828,  0.00359 time 65.4 sec\n",
      "Epoch 135, train  0.00965,  0.03822,  0.00361 time 65.3 sec\n",
      "Epoch 136, train  0.00959,  0.03802,  0.00360 time 65.1 sec\n",
      "Epoch 137, train  0.00958,  0.03789,  0.00359 time 65.2 sec\n",
      "Epoch 138, train  0.00953,  0.03777,  0.00359 time 65.3 sec\n",
      "Epoch 139, train  0.00951,  0.03766,  0.00359 time 65.1 sec\n",
      "Epoch 140, train  0.00946,  0.03748,  0.00358 time 65.3 sec\n",
      "Epoch 141, train  0.00941,  0.03734,  0.00357 time 65.1 sec\n",
      "Epoch 142, train  0.00944,  0.03728,  0.00359 time 65.0 sec\n",
      "Epoch 143, train  0.00937,  0.03709,  0.00357 time 65.3 sec\n",
      "Epoch 144, train  0.00932,  0.03696,  0.00356 time 65.1 sec\n",
      "Epoch 145, train  0.00932,  0.03683,  0.00357 time 65.3 sec\n",
      "Epoch 146, train  0.00928,  0.03671,  0.00356 time 65.1 sec\n",
      "Epoch 147, train  0.00920,  0.03653,  0.00354 time 65.3 sec\n",
      "Epoch 148, train  0.00916,  0.03640,  0.00353 time 65.2 sec\n",
      "Epoch 149, train  0.00919,  0.03632,  0.00355 time 65.2 sec\n",
      "Epoch 150, train  0.00917,  0.03622,  0.00356 time 65.5 sec\n",
      "Epoch 151, train  0.00910,  0.03606,  0.00354 time 65.5 sec\n",
      "Epoch 152, train  0.00904,  0.03591,  0.00352 time 65.2 sec\n",
      "Epoch 153, train  0.00901,  0.03575,  0.00352 time 65.2 sec\n",
      "Epoch 154, train  0.00900,  0.03565,  0.00352 time 65.2 sec\n",
      "Epoch 155, train  0.00897,  0.03550,  0.00352 time 65.2 sec\n",
      "Epoch 156, train  0.00894,  0.03541,  0.00352 time 65.2 sec\n",
      "Epoch 157, train  0.00889,  0.03525,  0.00351 time 65.3 sec\n",
      "Epoch 158, train  0.00886,  0.03518,  0.00351 time 65.3 sec\n",
      "Epoch 159, train  0.00886,  0.03506,  0.00351 time 65.1 sec\n",
      "Epoch 160, train  0.00882,  0.03494,  0.00351 time 65.0 sec\n",
      "Epoch 161, train  0.00875,  0.03476,  0.00349 time 65.1 sec\n",
      "Epoch 162, train  0.00875,  0.03467,  0.00350 time 65.2 sec\n",
      "Epoch 163, train  0.00873,  0.03454,  0.00350 time 65.4 sec\n",
      "Epoch 164, train  0.00872,  0.03443,  0.00350 time 65.3 sec\n",
      "Epoch 165, train  0.00865,  0.03426,  0.00348 time 65.2 sec\n",
      "Epoch 166, train  0.00858,  0.03415,  0.00346 time 65.3 sec\n",
      "Epoch 167, train  0.00859,  0.03409,  0.00348 time 65.0 sec\n",
      "Epoch 168, train  0.00853,  0.03389,  0.00346 time 65.1 sec\n",
      "Epoch 169, train  0.00852,  0.03380,  0.00346 time 65.2 sec\n",
      "Epoch 170, train  0.00847,  0.03366,  0.00346 time 65.3 sec\n",
      "Epoch 171, train  0.00845,  0.03357,  0.00346 time 65.2 sec\n",
      "Epoch 172, train  0.00843,  0.03343,  0.00345 time 65.2 sec\n",
      "Epoch 173, train  0.00840,  0.03331,  0.00345 time 65.4 sec\n",
      "Epoch 174, train  0.00837,  0.03321,  0.00345 time 65.5 sec\n",
      "Epoch 175, train  0.00834,  0.03309,  0.00344 time 65.1 sec\n",
      "Epoch 176, train  0.00833,  0.03298,  0.00345 time 65.2 sec\n",
      "Epoch 177, train  0.00827,  0.03284,  0.00343 time 65.2 sec\n",
      "Epoch 178, train  0.00825,  0.03275,  0.00344 time 65.5 sec\n",
      "Epoch 179, train  0.00821,  0.03263,  0.00343 time 65.4 sec\n",
      "Epoch 180, train  0.00819,  0.03253,  0.00343 time 65.5 sec\n",
      "Epoch 181, train  0.00816,  0.03244,  0.00343 time 65.3 sec\n",
      "Epoch 182, train  0.00811,  0.03228,  0.00342 time 65.4 sec\n",
      "Epoch 183, train  0.00807,  0.03216,  0.00341 time 65.2 sec\n",
      "Epoch 184, train  0.00805,  0.03204,  0.00341 time 65.4 sec\n",
      "Epoch 185, train  0.00800,  0.03193,  0.00339 time 65.3 sec\n",
      "Epoch 186, train  0.00801,  0.03183,  0.00341 time 65.4 sec\n",
      "Epoch 187, train  0.00795,  0.03170,  0.00339 time 65.3 sec\n",
      "Epoch 188, train  0.00794,  0.03158,  0.00340 time 65.4 sec\n",
      "Epoch 189, train  0.00789,  0.03146,  0.00338 time 65.2 sec\n",
      "Epoch 190, train  0.00788,  0.03137,  0.00339 time 65.2 sec\n",
      "Epoch 191, train  0.00783,  0.03126,  0.00338 time 65.5 sec\n",
      "Epoch 192, train  0.00779,  0.03113,  0.00337 time 65.2 sec\n",
      "Epoch 193, train  0.00776,  0.03102,  0.00337 time 65.5 sec\n",
      "Epoch 194, train  0.00777,  0.03097,  0.00338 time 64.9 sec\n",
      "Epoch 195, train  0.00773,  0.03085,  0.00337 time 65.3 sec\n",
      "Epoch 196, train  0.00770,  0.03074,  0.00337 time 65.2 sec\n",
      "Epoch 197, train  0.00767,  0.03060,  0.00337 time 65.2 sec\n",
      "Epoch 198, train  0.00761,  0.03045,  0.00335 time 65.1 sec\n",
      "Epoch 199, train  0.00757,  0.03035,  0.00335 time 65.1 sec\n",
      "Epoch 200, train  0.00756,  0.03029,  0.00335 time 65.4 sec\n",
      "Epoch 201, train  0.00753,  0.03015,  0.00334 time 65.3 sec\n",
      "Epoch 202, train  0.00755,  0.03011,  0.00336 time 65.2 sec\n",
      "Epoch 203, train  0.00745,  0.02990,  0.00333 time 65.0 sec\n",
      "Epoch 204, train  0.00744,  0.02983,  0.00333 time 65.3 sec\n",
      "Epoch 205, train  0.00740,  0.02970,  0.00333 time 65.3 sec\n",
      "Epoch 206, train  0.00740,  0.02967,  0.00334 time 65.0 sec\n",
      "Epoch 207, train  0.00732,  0.02949,  0.00331 time 64.9 sec\n",
      "Epoch 208, train  0.00738,  0.02950,  0.00334 time 65.1 sec\n",
      "Epoch 209, train  0.00730,  0.02931,  0.00332 time 65.1 sec\n",
      "Epoch 210, train  0.00725,  0.02925,  0.00331 time 65.0 sec\n",
      "Epoch 211, train  0.00723,  0.02908,  0.00331 time 65.3 sec\n",
      "Epoch 212, train  0.00724,  0.02903,  0.00332 time 65.2 sec\n",
      "Epoch 213, train  0.00719,  0.02889,  0.00331 time 65.1 sec\n",
      "Epoch 214, train  0.00715,  0.02878,  0.00330 time 65.2 sec\n",
      "Epoch 215, train  0.00714,  0.02870,  0.00330 time 65.4 sec\n",
      "Epoch 216, train  0.00710,  0.02860,  0.00330 time 65.6 sec\n",
      "Epoch 217, train  0.00707,  0.02847,  0.00328 time 65.7 sec\n",
      "Epoch 218, train  0.00703,  0.02836,  0.00328 time 65.5 sec\n",
      "Epoch 219, train  0.00702,  0.02827,  0.00329 time 65.6 sec\n",
      "Epoch 220, train  0.00698,  0.02818,  0.00327 time 65.2 sec\n",
      "Epoch 221, train  0.00695,  0.02808,  0.00327 time 65.0 sec\n",
      "Epoch 222, train  0.00693,  0.02798,  0.00328 time 65.1 sec\n",
      "Epoch 223, train  0.00688,  0.02786,  0.00326 time 65.1 sec\n",
      "Epoch 224, train  0.00688,  0.02780,  0.00327 time 65.2 sec\n",
      "Epoch 225, train  0.00687,  0.02774,  0.00327 time 65.5 sec\n",
      "Epoch 226, train  0.00685,  0.02761,  0.00327 time 65.1 sec\n",
      "Epoch 227, train  0.00679,  0.02748,  0.00326 time 65.2 sec\n",
      "Epoch 228, train  0.00675,  0.02736,  0.00324 time 65.1 sec\n",
      "Epoch 229, train  0.00675,  0.02729,  0.00325 time 65.1 sec\n",
      "Epoch 230, train  0.00668,  0.02717,  0.00323 time 65.0 sec\n",
      "Epoch 231, train  0.00669,  0.02712,  0.00324 time 65.0 sec\n",
      "Epoch 232, train  0.00666,  0.02700,  0.00324 time 65.0 sec\n",
      "Epoch 233, train  0.00661,  0.02688,  0.00323 time 65.4 sec\n",
      "Epoch 234, train  0.00662,  0.02683,  0.00323 time 65.1 sec\n",
      "Epoch 235, train  0.00657,  0.02670,  0.00322 time 65.3 sec\n",
      "Epoch 236, train  0.00656,  0.02663,  0.00323 time 65.1 sec\n",
      "Epoch 237, train  0.00654,  0.02654,  0.00322 time 65.1 sec\n",
      "Epoch 238, train  0.00649,  0.02642,  0.00322 time 65.3 sec\n",
      "Epoch 239, train  0.00646,  0.02633,  0.00320 time 65.2 sec\n",
      "Epoch 240, train  0.00642,  0.02623,  0.00320 time 65.1 sec\n",
      "Epoch 241, train  0.00642,  0.02615,  0.00320 time 65.1 sec\n",
      "Epoch 242, train  0.00639,  0.02608,  0.00320 time 65.1 sec\n",
      "Epoch 243, train  0.00632,  0.02594,  0.00319 time 65.0 sec\n",
      "Epoch 244, train  0.00637,  0.02589,  0.00321 time 65.1 sec\n",
      "Epoch 245, train  0.00633,  0.02581,  0.00320 time 65.2 sec\n",
      "Epoch 246, train  0.00627,  0.02568,  0.00319 time 65.3 sec\n",
      "Epoch 247, train  0.00626,  0.02563,  0.00319 time 65.2 sec\n",
      "Epoch 248, train  0.00622,  0.02549,  0.00318 time 65.2 sec\n",
      "Epoch 249, train  0.00619,  0.02539,  0.00317 time 65.3 sec\n",
      "Epoch 250, train  0.00618,  0.02533,  0.00317 time 65.2 sec\n",
      "Epoch 251, train  0.00613,  0.02521,  0.00316 time 65.1 sec\n",
      "Epoch 252, train  0.00612,  0.02515,  0.00316 time 65.2 sec\n",
      "Epoch 253, train  0.00611,  0.02504,  0.00317 time 65.2 sec\n",
      "Epoch 254, train  0.00608,  0.02497,  0.00317 time 65.2 sec\n",
      "Epoch 255, train  0.00604,  0.02485,  0.00315 time 65.2 sec\n",
      "Epoch 256, train  0.00599,  0.02476,  0.00314 time 65.2 sec\n",
      "Epoch 257, train  0.00600,  0.02471,  0.00315 time 65.0 sec\n",
      "Epoch 258, train  0.00597,  0.02463,  0.00315 time 65.1 sec\n",
      "Epoch 259, train  0.00594,  0.02453,  0.00314 time 65.0 sec\n",
      "Epoch 260, train  0.00588,  0.02442,  0.00312 time 65.4 sec\n",
      "Epoch 261, train  0.00590,  0.02437,  0.00313 time 65.2 sec\n",
      "Epoch 262, train  0.00587,  0.02424,  0.00313 time 65.1 sec\n",
      "Epoch 263, train  0.00583,  0.02416,  0.00312 time 65.2 sec\n",
      "Epoch 264, train  0.00581,  0.02408,  0.00312 time 65.2 sec\n",
      "Epoch 265, train  0.00577,  0.02399,  0.00311 time 65.2 sec\n",
      "Epoch 266, train  0.00574,  0.02390,  0.00311 time 65.4 sec\n",
      "Epoch 267, train  0.00575,  0.02385,  0.00311 time 65.3 sec\n",
      "Epoch 268, train  0.00572,  0.02375,  0.00311 time 65.1 sec\n",
      "Epoch 269, train  0.00569,  0.02368,  0.00311 time 64.9 sec\n",
      "Epoch 270, train  0.00565,  0.02356,  0.00309 time 65.0 sec\n",
      "Epoch 271, train  0.00564,  0.02351,  0.00309 time 66.2 sec\n",
      "Epoch 272, train  0.00562,  0.02340,  0.00309 time 68.5 sec\n",
      "Epoch 273, train  0.00562,  0.02334,  0.00310 time 65.2 sec\n",
      "Epoch 274, train  0.00559,  0.02324,  0.00309 time 65.2 sec\n",
      "Epoch 275, train  0.00556,  0.02319,  0.00308 time 65.2 sec\n",
      "Epoch 276, train  0.00554,  0.02311,  0.00309 time 65.0 sec\n",
      "Epoch 277, train  0.00549,  0.02300,  0.00307 time 65.0 sec\n",
      "Epoch 278, train  0.00550,  0.02295,  0.00308 time 65.1 sec\n",
      "Epoch 279, train  0.00543,  0.02279,  0.00306 time 66.0 sec\n",
      "Epoch 280, train  0.00546,  0.02278,  0.00308 time 65.5 sec\n",
      "Epoch 281, train  0.00541,  0.02267,  0.00306 time 65.0 sec\n",
      "Epoch 282, train  0.00539,  0.02259,  0.00307 time 65.2 sec\n",
      "Epoch 283, train  0.00536,  0.02253,  0.00306 time 64.9 sec\n",
      "Epoch 284, train  0.00531,  0.02242,  0.00305 time 65.2 sec\n",
      "Epoch 285, train  0.00532,  0.02236,  0.00305 time 65.2 sec\n",
      "Epoch 286, train  0.00530,  0.02229,  0.00305 time 65.3 sec\n",
      "Epoch 287, train  0.00528,  0.02221,  0.00304 time 65.5 sec\n",
      "Epoch 288, train  0.00526,  0.02213,  0.00305 time 65.7 sec\n",
      "Epoch 289, train  0.00522,  0.02204,  0.00303 time 65.5 sec\n",
      "Epoch 290, train  0.00521,  0.02195,  0.00304 time 65.5 sec\n",
      "Epoch 291, train  0.00519,  0.02191,  0.00303 time 65.3 sec\n",
      "Epoch 292, train  0.00516,  0.02183,  0.00303 time 65.0 sec\n",
      "Epoch 293, train  0.00513,  0.02175,  0.00301 time 65.3 sec\n",
      "Epoch 294, train  0.00511,  0.02166,  0.00302 time 65.2 sec\n",
      "Epoch 295, train  0.00507,  0.02156,  0.00301 time 65.2 sec\n",
      "Epoch 296, train  0.00504,  0.02148,  0.00299 time 65.3 sec\n",
      "Epoch 297, train  0.00504,  0.02139,  0.00301 time 65.4 sec\n",
      "Epoch 298, train  0.00504,  0.02136,  0.00301 time 65.1 sec\n",
      "Epoch 299, train  0.00502,  0.02130,  0.00301 time 65.0 sec\n",
      "Epoch 300, train  0.00499,  0.02120,  0.00301 time 65.2 sec\n",
      "Epoch 301, train  0.00497,  0.02115,  0.00300 time 65.3 sec\n",
      "Epoch 302, train  0.00494,  0.02105,  0.00299 time 65.2 sec\n",
      "Epoch 303, train  0.00489,  0.02097,  0.00298 time 65.1 sec\n",
      "Epoch 304, train  0.00488,  0.02090,  0.00298 time 65.0 sec\n",
      "Epoch 305, train  0.00487,  0.02082,  0.00298 time 65.0 sec\n",
      "Epoch 306, train  0.00484,  0.02074,  0.00298 time 65.0 sec\n",
      "Epoch 307, train  0.00485,  0.02071,  0.00298 time 65.0 sec\n",
      "Epoch 308, train  0.00481,  0.02063,  0.00297 time 65.1 sec\n",
      "Epoch 309, train  0.00477,  0.02050,  0.00296 time 65.3 sec\n",
      "Epoch 310, train  0.00473,  0.02043,  0.00295 time 65.1 sec\n",
      "Epoch 311, train  0.00473,  0.02037,  0.00296 time 65.1 sec\n",
      "Epoch 312, train  0.00473,  0.02035,  0.00295 time 65.1 sec\n",
      "Epoch 313, train  0.00469,  0.02025,  0.00295 time 65.1 sec\n",
      "Epoch 314, train  0.00468,  0.02020,  0.00295 time 65.1 sec\n",
      "Epoch 315, train  0.00467,  0.02011,  0.00295 time 65.1 sec\n",
      "Epoch 316, train  0.00468,  0.02008,  0.00295 time 65.3 sec\n",
      "Epoch 317, train  0.00463,  0.01997,  0.00294 time 65.2 sec\n",
      "Epoch 318, train  0.00459,  0.01990,  0.00293 time 65.0 sec\n",
      "Epoch 319, train  0.00456,  0.01980,  0.00292 time 64.8 sec\n",
      "Epoch 320, train  0.00456,  0.01976,  0.00293 time 65.0 sec\n",
      "Epoch 321, train  0.00455,  0.01968,  0.00293 time 65.3 sec\n",
      "Epoch 322, train  0.00453,  0.01963,  0.00293 time 65.1 sec\n",
      "Epoch 323, train  0.00448,  0.01954,  0.00291 time 65.1 sec\n",
      "Epoch 324, train  0.00450,  0.01953,  0.00292 time 65.1 sec\n",
      "Epoch 325, train  0.00451,  0.01945,  0.00294 time 65.0 sec\n",
      "Epoch 326, train  0.00443,  0.01933,  0.00290 time 65.2 sec\n",
      "Epoch 327, train  0.00442,  0.01927,  0.00291 time 65.2 sec\n",
      "Epoch 328, train  0.00439,  0.01918,  0.00290 time 65.1 sec\n",
      "Epoch 329, train  0.00437,  0.01912,  0.00289 time 65.2 sec\n",
      "Epoch 330, train  0.00439,  0.01912,  0.00291 time 65.2 sec\n",
      "Epoch 331, train  0.00436,  0.01905,  0.00289 time 65.1 sec\n",
      "Epoch 332, train  0.00433,  0.01896,  0.00289 time 65.0 sec\n",
      "Epoch 333, train  0.00431,  0.01888,  0.00289 time 65.2 sec\n",
      "Epoch 334, train  0.00430,  0.01884,  0.00288 time 65.1 sec\n",
      "Epoch 335, train  0.00427,  0.01876,  0.00287 time 65.1 sec\n",
      "Epoch 336, train  0.00428,  0.01872,  0.00289 time 65.1 sec\n",
      "Epoch 337, train  0.00424,  0.01862,  0.00287 time 65.1 sec\n",
      "Epoch 338, train  0.00420,  0.01855,  0.00286 time 65.3 sec\n",
      "Epoch 339, train  0.00420,  0.01851,  0.00287 time 65.2 sec\n",
      "Epoch 340, train  0.00421,  0.01847,  0.00288 time 65.3 sec\n",
      "Epoch 341, train  0.00419,  0.01842,  0.00286 time 65.2 sec\n",
      "Epoch 342, train  0.00413,  0.01831,  0.00285 time 65.2 sec\n",
      "Epoch 343, train  0.00411,  0.01825,  0.00284 time 64.9 sec\n",
      "Epoch 344, train  0.00411,  0.01819,  0.00285 time 64.9 sec\n",
      "Epoch 345, train  0.00412,  0.01816,  0.00286 time 65.2 sec\n",
      "Epoch 346, train  0.00408,  0.01805,  0.00285 time 65.0 sec\n",
      "Epoch 347, train  0.00405,  0.01800,  0.00284 time 64.9 sec\n",
      "Epoch 348, train  0.00406,  0.01796,  0.00285 time 65.3 sec\n",
      "Epoch 349, train  0.00403,  0.01791,  0.00283 time 65.1 sec\n",
      "Epoch 350, train  0.00400,  0.01781,  0.00283 time 65.2 sec\n",
      "Epoch 351, train  0.00397,  0.01772,  0.00282 time 65.1 sec\n",
      "Epoch 352, train  0.00398,  0.01770,  0.00283 time 65.2 sec\n",
      "Epoch 353, train  0.00393,  0.01761,  0.00281 time 65.2 sec\n",
      "Epoch 354, train  0.00392,  0.01757,  0.00281 time 65.1 sec\n",
      "Epoch 355, train  0.00395,  0.01758,  0.00282 time 65.2 sec\n",
      "Epoch 356, train  0.00390,  0.01745,  0.00281 time 65.1 sec\n",
      "Epoch 357, train  0.00389,  0.01744,  0.00281 time 65.5 sec\n",
      "Epoch 358, train  0.00388,  0.01733,  0.00281 time 65.3 sec\n",
      "Epoch 359, train  0.00386,  0.01727,  0.00280 time 65.0 sec\n",
      "Epoch 360, train  0.00382,  0.01718,  0.00279 time 65.1 sec\n",
      "Epoch 361, train  0.00381,  0.01715,  0.00279 time 65.0 sec\n",
      "Epoch 362, train  0.00381,  0.01711,  0.00279 time 65.2 sec\n",
      "Epoch 363, train  0.00380,  0.01705,  0.00279 time 65.4 sec\n",
      "Epoch 364, train  0.00378,  0.01701,  0.00279 time 65.5 sec\n",
      "Epoch 365, train  0.00378,  0.01697,  0.00279 time 65.4 sec\n",
      "Epoch 366, train  0.00373,  0.01688,  0.00277 time 65.4 sec\n",
      "Epoch 367, train  0.00372,  0.01682,  0.00278 time 65.1 sec\n",
      "Epoch 368, train  0.00372,  0.01677,  0.00278 time 64.9 sec\n",
      "Epoch 369, train  0.00370,  0.01671,  0.00277 time 65.0 sec\n",
      "Epoch 370, train  0.00365,  0.01662,  0.00276 time 64.8 sec\n",
      "Epoch 371, train  0.00367,  0.01659,  0.00277 time 65.3 sec\n",
      "Epoch 372, train  0.00366,  0.01657,  0.00276 time 65.5 sec\n",
      "Epoch 373, train  0.00363,  0.01649,  0.00275 time 65.5 sec\n",
      "Epoch 374, train  0.00360,  0.01640,  0.00275 time 65.3 sec\n",
      "Epoch 375, train  0.00357,  0.01633,  0.00274 time 65.3 sec\n",
      "Epoch 376, train  0.00358,  0.01633,  0.00275 time 65.5 sec\n",
      "Epoch 377, train  0.00356,  0.01626,  0.00274 time 65.5 sec\n",
      "Epoch 378, train  0.00355,  0.01621,  0.00274 time 65.3 sec\n",
      "Epoch 379, train  0.00352,  0.01613,  0.00273 time 65.1 sec\n",
      "Epoch 380, train  0.00354,  0.01610,  0.00274 time 65.1 sec\n",
      "Epoch 381, train  0.00354,  0.01610,  0.00275 time 65.0 sec\n",
      "Epoch 382, train  0.00350,  0.01600,  0.00273 time 65.0 sec\n",
      "Epoch 383, train  0.00348,  0.01595,  0.00272 time 65.1 sec\n",
      "Epoch 384, train  0.00344,  0.01588,  0.00272 time 65.0 sec\n",
      "Epoch 385, train  0.00345,  0.01585,  0.00272 time 65.1 sec\n",
      "Epoch 386, train  0.00342,  0.01577,  0.00271 time 65.2 sec\n",
      "Epoch 387, train  0.00343,  0.01574,  0.00272 time 65.3 sec\n",
      "Epoch 388, train  0.00341,  0.01569,  0.00271 time 65.3 sec\n",
      "Epoch 389, train  0.00343,  0.01567,  0.00272 time 65.1 sec\n",
      "Epoch 390, train  0.00336,  0.01556,  0.00269 time 65.1 sec\n",
      "Epoch 391, train  0.00337,  0.01555,  0.00270 time 65.4 sec\n",
      "Epoch 392, train  0.00335,  0.01549,  0.00269 time 65.2 sec\n",
      "Epoch 393, train  0.00334,  0.01544,  0.00269 time 65.1 sec\n",
      "Epoch 394, train  0.00332,  0.01538,  0.00269 time 65.1 sec\n",
      "Epoch 395, train  0.00329,  0.01530,  0.00268 time 65.0 sec\n",
      "Epoch 396, train  0.00327,  0.01526,  0.00267 time 65.1 sec\n",
      "Epoch 397, train  0.00329,  0.01525,  0.00268 time 65.0 sec\n",
      "Epoch 398, train  0.00327,  0.01517,  0.00268 time 65.2 sec\n",
      "Epoch 399, train  0.00324,  0.01511,  0.00267 time 65.3 sec\n",
      "Epoch 400, train  0.00321,  0.01505,  0.00266 time 65.0 sec\n",
      "Epoch 401, train  0.00323,  0.01503,  0.00266 time 65.3 sec\n",
      "Epoch 402, train  0.00320,  0.01497,  0.00266 time 65.1 sec\n",
      "Epoch 403, train  0.00319,  0.01492,  0.00266 time 65.1 sec\n",
      "Epoch 404, train  0.00318,  0.01487,  0.00266 time 65.1 sec\n",
      "Epoch 405, train  0.00317,  0.01480,  0.00266 time 65.2 sec\n",
      "Epoch 406, train  0.00316,  0.01478,  0.00265 time 64.9 sec\n",
      "Epoch 407, train  0.00314,  0.01472,  0.00265 time 65.0 sec\n",
      "Epoch 408, train  0.00313,  0.01470,  0.00264 time 65.2 sec\n",
      "Epoch 409, train  0.00311,  0.01464,  0.00263 time 65.2 sec\n",
      "Epoch 410, train  0.00311,  0.01459,  0.00264 time 65.4 sec\n",
      "Epoch 411, train  0.00311,  0.01455,  0.00264 time 65.1 sec\n",
      "Epoch 412, train  0.00307,  0.01450,  0.00263 time 65.3 sec\n",
      "Epoch 413, train  0.00309,  0.01448,  0.00264 time 65.1 sec\n",
      "Epoch 414, train  0.00306,  0.01441,  0.00264 time 65.1 sec\n",
      "Epoch 415, train  0.00305,  0.01436,  0.00263 time 65.3 sec\n",
      "Epoch 416, train  0.00300,  0.01430,  0.00261 time 65.3 sec\n",
      "Epoch 417, train  0.00300,  0.01423,  0.00261 time 65.3 sec\n",
      "Epoch 418, train  0.00303,  0.01424,  0.00262 time 65.4 sec\n",
      "Epoch 419, train  0.00299,  0.01417,  0.00261 time 65.6 sec\n",
      "Epoch 420, train  0.00298,  0.01414,  0.00261 time 65.2 sec\n",
      "Epoch 421, train  0.00297,  0.01411,  0.00260 time 65.1 sec\n",
      "Epoch 422, train  0.00296,  0.01405,  0.00260 time 65.0 sec\n",
      "Epoch 423, train  0.00293,  0.01397,  0.00260 time 65.3 sec\n",
      "Epoch 424, train  0.00293,  0.01396,  0.00259 time 65.1 sec\n",
      "Epoch 425, train  0.00288,  0.01385,  0.00257 time 65.1 sec\n",
      "Epoch 426, train  0.00290,  0.01386,  0.00258 time 65.3 sec\n",
      "Epoch 427, train  0.00290,  0.01382,  0.00259 time 65.2 sec\n",
      "Epoch 428, train  0.00286,  0.01378,  0.00257 time 65.3 sec\n",
      "Epoch 429, train  0.00287,  0.01374,  0.00258 time 65.1 sec\n",
      "Epoch 430, train  0.00286,  0.01369,  0.00257 time 65.2 sec\n",
      "Epoch 431, train  0.00287,  0.01367,  0.00259 time 65.1 sec\n",
      "Epoch 432, train  0.00286,  0.01364,  0.00258 time 65.2 sec\n",
      "Epoch 433, train  0.00281,  0.01357,  0.00255 time 65.2 sec\n",
      "Epoch 434, train  0.00280,  0.01352,  0.00256 time 65.1 sec\n",
      "Epoch 435, train  0.00279,  0.01346,  0.00255 time 65.4 sec\n",
      "Epoch 436, train  0.00281,  0.01346,  0.00257 time 65.9 sec\n",
      "Epoch 437, train  0.00280,  0.01341,  0.00256 time 65.7 sec\n",
      "Epoch 438, train  0.00277,  0.01333,  0.00255 time 65.5 sec\n",
      "Epoch 439, train  0.00276,  0.01331,  0.00255 time 65.2 sec\n",
      "Epoch 440, train  0.00274,  0.01327,  0.00254 time 65.3 sec\n",
      "Epoch 441, train  0.00274,  0.01325,  0.00254 time 65.2 sec\n",
      "Epoch 442, train  0.00274,  0.01321,  0.00254 time 65.1 sec\n",
      "Epoch 443, train  0.00273,  0.01316,  0.00254 time 65.1 sec\n",
      "Epoch 444, train  0.00270,  0.01312,  0.00253 time 65.1 sec\n",
      "Epoch 445, train  0.00270,  0.01307,  0.00253 time 65.2 sec\n",
      "Epoch 446, train  0.00269,  0.01305,  0.00253 time 65.1 sec\n",
      "Epoch 447, train  0.00271,  0.01303,  0.00254 time 65.3 sec\n",
      "Epoch 448, train  0.00266,  0.01294,  0.00252 time 65.3 sec\n",
      "Epoch 449, train  0.00268,  0.01295,  0.00253 time 65.4 sec\n",
      "Epoch 450, train  0.00265,  0.01289,  0.00251 time 65.1 sec\n",
      "Epoch 451, train  0.00264,  0.01283,  0.00252 time 65.2 sec\n",
      "Epoch 452, train  0.00259,  0.01275,  0.00250 time 65.4 sec\n",
      "Epoch 453, train  0.00262,  0.01276,  0.00251 time 65.3 sec\n",
      "Epoch 454, train  0.00261,  0.01274,  0.00250 time 65.2 sec\n",
      "Epoch 455, train  0.00259,  0.01267,  0.00250 time 65.2 sec\n",
      "Epoch 456, train  0.00258,  0.01266,  0.00249 time 65.1 sec\n",
      "Epoch 457, train  0.00258,  0.01261,  0.00250 time 65.1 sec\n",
      "Epoch 458, train  0.00257,  0.01259,  0.00249 time 65.1 sec\n",
      "Epoch 459, train  0.00256,  0.01253,  0.00249 time 65.5 sec\n",
      "Epoch 460, train  0.00252,  0.01246,  0.00247 time 65.3 sec\n",
      "Epoch 461, train  0.00254,  0.01247,  0.00249 time 65.3 sec\n",
      "Epoch 462, train  0.00252,  0.01242,  0.00247 time 65.3 sec\n",
      "Epoch 463, train  0.00252,  0.01238,  0.00248 time 65.3 sec\n",
      "Epoch 464, train  0.00251,  0.01235,  0.00247 time 65.2 sec\n",
      "Epoch 465, train  0.00250,  0.01230,  0.00247 time 65.1 sec\n",
      "Epoch 466, train  0.00247,  0.01224,  0.00246 time 65.2 sec\n",
      "Epoch 467, train  0.00246,  0.01220,  0.00245 time 65.5 sec\n",
      "Epoch 468, train  0.00247,  0.01219,  0.00246 time 65.3 sec\n",
      "Epoch 469, train  0.00246,  0.01216,  0.00246 time 65.2 sec\n",
      "Epoch 470, train  0.00247,  0.01216,  0.00246 time 65.1 sec\n",
      "Epoch 471, train  0.00246,  0.01211,  0.00245 time 65.0 sec\n",
      "Epoch 472, train  0.00243,  0.01204,  0.00245 time 65.1 sec\n",
      "Epoch 473, train  0.00244,  0.01204,  0.00245 time 65.4 sec\n",
      "Epoch 474, train  0.00242,  0.01199,  0.00245 time 65.1 sec\n",
      "Epoch 475, train  0.00242,  0.01196,  0.00244 time 65.2 sec\n",
      "Epoch 476, train  0.00237,  0.01187,  0.00242 time 65.1 sec\n",
      "Epoch 477, train  0.00237,  0.01184,  0.00242 time 65.2 sec\n",
      "Epoch 478, train  0.00237,  0.01184,  0.00243 time 65.2 sec\n",
      "Epoch 479, train  0.00236,  0.01180,  0.00242 time 65.1 sec\n",
      "Epoch 480, train  0.00236,  0.01178,  0.00242 time 65.1 sec\n",
      "Epoch 481, train  0.00233,  0.01170,  0.00241 time 65.2 sec\n",
      "Epoch 482, train  0.00234,  0.01170,  0.00241 time 65.3 sec\n",
      "Epoch 483, train  0.00233,  0.01166,  0.00241 time 65.1 sec\n",
      "Epoch 484, train  0.00234,  0.01165,  0.00242 time 65.2 sec\n",
      "Epoch 485, train  0.00230,  0.01158,  0.00240 time 65.2 sec\n",
      "Epoch 486, train  0.00231,  0.01157,  0.00241 time 65.2 sec\n",
      "Epoch 487, train  0.00229,  0.01151,  0.00240 time 65.2 sec\n",
      "Epoch 488, train  0.00229,  0.01149,  0.00240 time 65.1 sec\n",
      "Epoch 489, train  0.00227,  0.01143,  0.00239 time 65.2 sec\n",
      "Epoch 490, train  0.00228,  0.01145,  0.00239 time 65.3 sec\n",
      "Epoch 491, train  0.00227,  0.01138,  0.00239 time 65.4 sec\n",
      "Epoch 492, train  0.00224,  0.01135,  0.00238 time 65.2 sec\n",
      "Epoch 493, train  0.00224,  0.01132,  0.00238 time 65.3 sec\n",
      "Epoch 494, train  0.00224,  0.01130,  0.00238 time 65.1 sec\n",
      "Epoch 495, train  0.00222,  0.01126,  0.00238 time 65.4 sec\n",
      "Epoch 496, train  0.00222,  0.01123,  0.00237 time 65.1 sec\n",
      "Epoch 497, train  0.00220,  0.01117,  0.00237 time 65.1 sec\n",
      "Epoch 498, train  0.00222,  0.01118,  0.00238 time 65.2 sec\n",
      "Epoch 499, train  0.00221,  0.01115,  0.00237 time 65.3 sec\n"
     ]
    }
   ],
   "source": [
    "from mxnet import init\n",
    "from mxnet import gpu\n",
    "\n",
    "positive_weight = 3.0\n",
    "negative_weight = 0.1\n",
    "class_weight = 1.0\n",
    "box_weight = 3.0\n",
    "\n",
    "ctx = gpu(0)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.001, 'wd': 5e-4})\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from mxnet import autograd\n",
    "for epoch in range(500):\n",
    "    # reset data iterators and metrics\n",
    "    train_data.reset()\n",
    "    cls_loss.reset()\n",
    "    obj_loss.reset()\n",
    "    box_loss.reset()\n",
    "    tic = time.time()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        x = batch.data[0].as_in_context(ctx)\n",
    "        y = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            x = net(x)\n",
    "            output, cls_pred, score, xywh = yolo2_forward(x, num_class, scales)\n",
    "            with autograd.pause():\n",
    "                tid, tscore, tbox, sample_weight = yolo2_target(score, xywh, y, scales, thresh=0.5)\n",
    "            # losses\n",
    "            loss1 = sce_loss(cls_pred, tid, sample_weight * class_weight)\n",
    "            score_weight = nd.where(sample_weight > 0,\n",
    "                                    nd.ones_like(sample_weight) * positive_weight,\n",
    "                                    nd.ones_like(sample_weight) * negative_weight)\n",
    "            loss2 = l1_loss(score, tscore, score_weight)\n",
    "            loss3 = l1_loss(xywh, tbox, sample_weight * box_weight)\n",
    "            loss = loss1 + loss2 + loss3\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        # update metrics\n",
    "        cls_loss.update(loss1)\n",
    "        obj_loss.update(loss2)\n",
    "        box_loss.update(loss3)\n",
    "\n",
    "    print('Epoch %2d, train  %.5f,  %.5f,  %.5f time %.1f sec' % (\n",
    "        epoch, cls_loss.get()[-1], obj_loss.get()[-1], box_loss.get()[-1], time.time()-tic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 1568L, 6L)\n"
     ]
    }
   ],
   "source": [
    "def process_image(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        im = image.imdecode(f.read())\n",
    "    # resize to data_shape\n",
    "    data = image.imresize(im, data_shape, data_shape)\n",
    "    # minus rgb mean, divide std\n",
    "    data = (data.astype('float32') - rgb_mean) / rgb_std\n",
    "    # convert to batch x channel x height xwidth\n",
    "    return data.transpose((2,0,1)).expand_dims(axis=0), im\n",
    "\n",
    "def predict(x):\n",
    "    x = net(x)\n",
    "    output, cls_prob, score, xywh = yolo2_forward(x, num_class, scales)\n",
    "    return nd.contrib.box_nms(output.reshape((0, -1, 6)))\n",
    "\n",
    "x, im = process_image('D:/1/2.jpg')\n",
    "out = predict(x.as_in_context(ctx))\n",
    "print out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mpl.rcParams['figure.figsize'] = (6,6)\n",
    "import matplotlib.pyplot as plt\n",
    "colors = ['blue', 'green', 'red', 'black', 'magenta','yellow']\n",
    "\n",
    "def box_to_rect(box, color, linewidth=3):\n",
    "    \"\"\"convert an anchor box to a matplotlib rectangle\"\"\"\n",
    "    box = box.asnumpy()\n",
    "    return plt.Rectangle(\n",
    "        (box[0], box[1]), (box[2]-box[0]), (box[3]-box[1]),\n",
    "        fill=False, edgecolor=color, linewidth=linewidth)\n",
    "\n",
    "\n",
    "def display(im, out, threshold=0.5):\n",
    "    plt.imshow(im.asnumpy())\n",
    "    for row in out:\n",
    "        row = row.asnumpy()\n",
    "        class_id, score = int(row[0]), row[1]\n",
    "        if class_id < 0 or score < threshold:\n",
    "            continue\n",
    "        color = colors[class_id%len(colors)]\n",
    "        box = row[2:6] * np.array([im.shape[1],im.shape[0]]*2)\n",
    "        rect = box_to_rect(nd.array(box), color, 2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[class_id]\n",
    "        plt.gca().text(box[0], box[1],\n",
    "                       '{:s} {:.2f}'.format(text, score),\n",
    "                       bbox=dict(facecolor=color, alpha=0.5),\n",
    "                       fontsize=10, color='white')\n",
    "    plt.show()\n",
    "\n",
    "display(im, out[0], threshold=0.92)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#net.save_params('')\n",
    "net.save_parameters('500epoch.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
